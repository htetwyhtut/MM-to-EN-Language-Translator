{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 Import libraries and define device to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random, math, time\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1\n",
    "Find a dataset suitable for translation between your native language and English. Ensure to source this dataset from reputable public databases or repositories. It is imperative to give proper credit to the dataset source in your documentation. (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Note**: As a Myanmar Nationality, I choose ENGLISH - MYANMAR text pair as my dataset.\n",
    "\n",
    "- I obtained my dataset from https://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/\n",
    "\n",
    "- The copyright holder of the translations of the ALT Parallel Corpus is the National Institute of Information and Communications Technology, Japan (NICT). \n",
    "\n",
    "- The University of Computer Studies, Yangon, Myanmar helped NICT translate the English texts into the Myanmar texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 18088\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "        num_rows: 1019\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['SNT.URLID', 'SNT.URLID.SNTID', 'url', 'translation'],\n",
       "    num_rows: 18088\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SNT.URLID': '87564',\n",
       " 'SNT.URLID.SNTID': '13',\n",
       " 'url': 'http://en.wikinews.org/wiki/Data_for_3_million_UK_driving_candidates_lost',\n",
       " 'translation': {'bg': 'অক্টোবরে এইচএম রেভিনিউ ২৫ মিলিয়ন লোকের তথ্য হারিয়ে ফেলার পরে এটিই হল ইউকে-তে প্রথম এত বড় তথ্যের ক্ষতি।',\n",
       "  'en': 'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October.',\n",
       "  'en_tok': 'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October .',\n",
       "  'fil': 'Ito ang unang malakihang pagkawala ng data sa UK dahil ang impormasyon sa 25 milyong tao ay nawalan ng HM Revenue noong Oktubre.',\n",
       "  'hi': 'यह ब्रिटेन में डेटा का पहला बड़ा नुकसान है क्योंकि अक्टूबर में HM रेवेन्यू द्वारा 25 मिलियन लोगों की जानकारी गुम हो गई थी।',\n",
       "  'id': 'Ini adalah kehilangan data yang besar pertama di UK sejak hilangnya informasi tentang 25 juta orang oleh HM Revenue di bulan Oktober.',\n",
       "  'ja': 'これは、10月に歳入関税庁が2500万人分の情報を失って以来初めてのイギリスでの大きなデータ紛失である。',\n",
       "  'khm': 'នេះជាការបាត់បង់ទិន្នន័យដ៏ធំំលើកទីមួយក្នុងចក្រភពអង់គ្លេសបន្ទាប់ពីបានបាត់បង់ព័ត៌មាន25លាននាក់នៅទីភ្នាក់ងារចំណូលគយនៅខែតុលា។',\n",
       "  'lo': 'ນີ້ແມ່ນການສູນເສຍຄັ້ງສຳຄັນຂອງຂໍ້ມູນໃນ ສະຫະລາຊະອານາຈັກ ຕັ້ງແຕ່ຂໍ້ມູນຂອງ 25 ລ້ານຄົນ ໄດ້ສູນຫາຍໄປໂດຍ HM ເຮເວີນູ ໃນເດືອນຕຸລາ.',\n",
       "  'ms': 'Ini adalah kehilangan data terbesar di UK semenjak maklumat 25 juta orang dihilangkan oleh HM Revenue pada Oktober.',\n",
       "  'my': 'အဆိုပါကိစ္စ သည် အောက်တိုဘာလ တွင် လူ ၂၅ သန်း ၏ သတင်းအချက်အလက်များ အခွန်ဝန်ကြီးဌာန ကြောင့် ဆုံးရှုံးခဲ့ ကတည်းက ဗြိတိန်နိုင်ငံ တွင် ပထမဦးဆုံးသော အဓိကကျသည့် အချက်အလက်များ ၏ ပျောက်ဆုံးမှု ဖြစ်သည် ။',\n",
       "  'th': 'นับเป็นการสูญหายของข้อมูลครั้งใหญ่ครั้งแรกในUK ตั้งแต่ข้อมูลของประชาชน25 ล้านคนสูญหายโดยHMของสรรพากรเมื่อเดือนตุลาคม',\n",
       "  'vi': 'Đây là vụ mất dữ liệu lớn đầu tiên tại Anh kể từ khi Cục Thuế làm mất dữ liệu của 25 triệu người vào tháng Mười.',\n",
       "  'zh': '这是自去年10月英国税务机构丢失2500万人信息以来，英国首次出现重大数据丢失。'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bg': 'অক্টোবরে এইচএম রেভিনিউ ২৫ মিলিয়ন লোকের তথ্য হারিয়ে ফেলার পরে এটিই হল ইউকে-তে প্রথম এত বড় তথ্যের ক্ষতি।',\n",
       " 'en': 'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October.',\n",
       " 'en_tok': 'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October .',\n",
       " 'fil': 'Ito ang unang malakihang pagkawala ng data sa UK dahil ang impormasyon sa 25 milyong tao ay nawalan ng HM Revenue noong Oktubre.',\n",
       " 'hi': 'यह ब्रिटेन में डेटा का पहला बड़ा नुकसान है क्योंकि अक्टूबर में HM रेवेन्यू द्वारा 25 मिलियन लोगों की जानकारी गुम हो गई थी।',\n",
       " 'id': 'Ini adalah kehilangan data yang besar pertama di UK sejak hilangnya informasi tentang 25 juta orang oleh HM Revenue di bulan Oktober.',\n",
       " 'ja': 'これは、10月に歳入関税庁が2500万人分の情報を失って以来初めてのイギリスでの大きなデータ紛失である。',\n",
       " 'khm': 'នេះជាការបាត់បង់ទិន្នន័យដ៏ធំំលើកទីមួយក្នុងចក្រភពអង់គ្លេសបន្ទាប់ពីបានបាត់បង់ព័ត៌មាន25លាននាក់នៅទីភ្នាក់ងារចំណូលគយនៅខែតុលា។',\n",
       " 'lo': 'ນີ້ແມ່ນການສູນເສຍຄັ້ງສຳຄັນຂອງຂໍ້ມູນໃນ ສະຫະລາຊະອານາຈັກ ຕັ້ງແຕ່ຂໍ້ມູນຂອງ 25 ລ້ານຄົນ ໄດ້ສູນຫາຍໄປໂດຍ HM ເຮເວີນູ ໃນເດືອນຕຸລາ.',\n",
       " 'ms': 'Ini adalah kehilangan data terbesar di UK semenjak maklumat 25 juta orang dihilangkan oleh HM Revenue pada Oktober.',\n",
       " 'my': 'အဆိုပါကိစ္စ သည် အောက်တိုဘာလ တွင် လူ ၂၅ သန်း ၏ သတင်းအချက်အလက်များ အခွန်ဝန်ကြီးဌာန ကြောင့် ဆုံးရှုံးခဲ့ ကတည်းက ဗြိတိန်နိုင်ငံ တွင် ပထမဦးဆုံးသော အဓိကကျသည့် အချက်အလက်များ ၏ ပျောက်ဆုံးမှု ဖြစ်သည် ။',\n",
       " 'th': 'นับเป็นการสูญหายของข้อมูลครั้งใหญ่ครั้งแรกในUK ตั้งแต่ข้อมูลของประชาชน25 ล้านคนสูญหายโดยHMของสรรพากรเมื่อเดือนตุลาคม',\n",
       " 'vi': 'Đây là vụ mất dữ liệu lớn đầu tiên tại Anh kể từ khi Cục Thuế làm mất dữ liệu của 25 triệu người vào tháng Mười.',\n",
       " 'zh': '这是自去年10月英国税务机构丢失2500万人信息以来，英国首次出现重大数据丢失。'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][12]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is the first major loss of data in the UK since information on 25 million people was lost by HM Revenue in October.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][12]['translation']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'အဆိုပါကိစ္စ သည် အောက်တိုဘာလ တွင် လူ ၂၅ သန်း ၏ သတင်းအချက်အလက်များ အခွန်ဝန်ကြီးဌာန ကြောင့် ဆုံးရှုံးခဲ့ ကတည်းက ဗြိတိန်နိုင်ငံ တွင် ပထမဦးဆုံးသော အဓိကကျသည့် အချက်အလက်များ ၏ ပျောက်ဆုံးမှု ဖြစ်သည် ။'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][12]['translation']['my']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2\n",
    "Describe in detail the process of preparing the dataset for use in your translation model. This includes steps like text normalization, tokenization, and word segmentation, particularly focusing on your native language’s specific requirements. Specify the libraries or tools you will use for these tasks and give appropriate credit to the developers or organizations behind these tools. If your native language requires special handling in tokenization (e.g., for languages like Chinese, Thai, or Japanese), mention the libraries (like Jieba, PyThaiNLP, or Mecab) and the procedures used for word segmentation. (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Initially the dataset contains english and multiple Asian language pairs. I firstly split it into English and Myanmar pair.\n",
    "2. ENGLISH tokenization done using spacy tokenizer with the en_core_web_sm model.\n",
    "3. MYANMAR tokenization done using custom tokenizer for Myanmar using the Viterbi algorithm for word segmentation. This is a sophisticated approach and well-suited for Myanmar, which requires word segmentation. (Full credit to Dr. Ye Kyaw Thu for the Viterbi algorithm implementation. [https://github.com/ye-kyaw-thu/myWord] )\n",
    "4. Vocabulary buidling: Tokenized English and Myanmar text are used to build a vocabulary. The vocabulary is then used to convert text into numerical indices for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset of English and Myanmar pair\n",
    "\n",
    "datasetENMY = {}\n",
    "\n",
    "# Define source and target languages\n",
    "SRC_LANGUAGE = 'en'  # English\n",
    "TRG_LANGUAGE = 'my'  # Myanmar\n",
    "languages   = [SRC_LANGUAGE, TRG_LANGUAGE]\n",
    "\n",
    "for data in dataset:\n",
    "# english myanmar data\n",
    "    datasetENMY[data] = [{lang: row['translation'][lang] for lang in languages} for row in dataset[data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 18088\n",
      "validation : 1000\n",
      "test : 1019\n"
     ]
    }
   ],
   "source": [
    "# check the size for each dataset\n",
    "for data in datasetENMY:\n",
    "    print(f\"{data} : {len(datasetENMY[data])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = datasetENMY['train'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The TimesOnline reports that the joke fell flat with Jeffrey Turner, who as Chief of Police in Clayton County, Georgia, put Mr Whitton on medical leave when he was shot in the wrist as he tried to foil a robbery earlier this summer.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[SRC_LANGUAGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ယခု နွေရာသီ အစောပိုင်း ကာလ ၌ လုယက်မှု တစ်ခု ကျူးလွန် ရန် ကြိုးစားခဲ့ သောကြောင့် လက်ကောက်၀တ် တွင် သေနတ်ကျည်မှန်ခဲ့သော မစ်စတာ ၀ှစ်တွန် ကို ကလေတွန် ကောင်တီ ၊ ဂျော်ဂျီယာပြည်နယ် မှ ၊ ရဲမှူးကြီး ဂျက်ဖရီ တာနာ မှ ဆေး ခွင့် ပေးတာနှင့် ပတ်သတ်ပြီး ၊ တိုင်းမ်စ်အွန်လိုင်း မှ ဟာသပြက်လုံးတစ်ခု ကို မှတ်တမ်းတင်ရေးသားခဲ့သည် ။'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[TRG_LANGUAGE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English language tokenization\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  The TimesOnline reports that the joke fell flat with Jeffrey Turner, who as Chief of Police in Clayton County, Georgia, put Mr Whitton on medical leave when he was shot in the wrist as he tried to foil a robbery earlier this summer.\n",
      "Tokenization:  ['The', 'TimesOnline', 'reports', 'that', 'the', 'joke', 'fell', 'flat', 'with', 'Jeffrey', 'Turner', ',', 'who', 'as', 'Chief', 'of', 'Police', 'in', 'Clayton', 'County', ',', 'Georgia', ',', 'put', 'Mr', 'Whitton', 'on', 'medical', 'leave', 'when', 'he', 'was', 'shot', 'in', 'the', 'wrist', 'as', 'he', 'tried', 'to', 'foil', 'a', 'robbery', 'earlier', 'this', 'summer', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[SRC_LANGUAGE])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[SRC_LANGUAGE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Myanmar language custom tokenization\n",
    "\"\"\"\n",
    "\n",
    "This code is from https://github.com/ye-kyaw-thu/myWord/blob/main/word_segment.py\n",
    "Full Creit to Dr.Ye Kyaw Thu.\n",
    "\n",
    "Ye Kyaw Thu (Ye-san) is a Visiting Professor of Language & Semantic Technology Research Team (LST), Artificial Intelligence Research Unit (AINRU), National Electronic & Computer Technology Center (NECTEC), Thailand, Affiliate Professor at Cambodia Academy of Digital Technology (CADT), Cambodia and Head of NLP Research Lab., University of Technology Yatanarpon Cyber City (UTYCC), Pyin Oo Lwin, Myanmar.\n",
    "\n",
    "# References:\n",
    "- Python implementation of Viterbi algorithm for word segmentation: \n",
    "- Updated version of this: https://gist.github.com/markdtw/e2a4e2ee7cef8ea6aed33bb47a97fba6\n",
    "- A clean-up of this: http://norvig.com/ngrams/ch14.pdf\n",
    "- For recursion limit: https://www.geeksforgeeks.org/python-handling-recursion-limit/\n",
    "- A. Viterbi, \"Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,\" in IEEE Transactions on Information Theory, vol. 13, no. 2, pp. 260-269, April 1967, doi: 10.1109/TIT.1967.1054010.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import functools\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.setrecursionlimit(10**6)\n",
    "\n",
    "# unigram and bigram dictionary from https://github.com/ye-kyaw-thu/myWord/tree/main/dict_ver1\n",
    "uni_dict_bin = '../app/data/unigram-word.bin'\n",
    "bi_dict_bin = '../app/data/bigram-word.bin'                \n",
    "\n",
    "def read_dict (fileDICT):\n",
    "    try:\n",
    "        with open(fileDICT, 'rb') as input_file:\n",
    "            dictionary = pickle.load(input_file)\n",
    "            input_file.close()\n",
    "    except FileNotFoundError:\n",
    "        print('Dictionary file', fileDICT, ' not found!')\n",
    "    return dictionary\n",
    "\n",
    "class ProbDist(dict):\n",
    "    ### Probability distribution estimated from unigram/bigram data\n",
    "    def __init__(self, datafile=None, unigram=True, N=102490):\n",
    "    #def __init__(self, datafile=None, unigram=True, N=1024908267229):\n",
    "    #def __init__(self, datafile=None, unigram=True, N=8199266137832):\n",
    "        #data = {}\n",
    "        data = read_dict(datafile)\n",
    "        for k, c in data.items():\n",
    "            self[k] = self.get(k, 0) + c\n",
    "\n",
    "        if unigram:\n",
    "            self.unknownprob = lambda k, N: 10 / (N*10**len(k))    # avoid unknown long word\n",
    "        else:\n",
    "            self.unknownprob = lambda k, N: 1 / N\n",
    "\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]/self.N\n",
    "        else:\n",
    "            return self.unknownprob(key, self.N)\n",
    "        \n",
    "\n",
    "P_unigram = ProbDist(uni_dict_bin, True)\n",
    "P_bigram = ProbDist(bi_dict_bin, False)\n",
    "\n",
    "\n",
    "def conditionalProb(word_curr, word_prev):\n",
    "    ### Conditional probability of current word given the previous word.\n",
    "    try:\n",
    "        return P_bigram[word_prev + ' ' + word_curr]/P_unigram[word_prev]\n",
    "    except KeyError:\n",
    "        return P_unigram(word_curr)\n",
    "\n",
    "\n",
    "@functools.lru_cache(maxsize=2**10)\n",
    "#maxlen=20\n",
    "def viterbi(text, prev='<S>', maxlen=20):\n",
    "    if not text:\n",
    "        return 0.0, []\n",
    "    \n",
    "    #print(\"text: \", text)\n",
    "    textlen = min(len(text), maxlen)\n",
    "    splits = [(text[:i + 1], text[i + 1:]) for i in range(textlen)]\n",
    "\n",
    "    candidates = []\n",
    "    #print(\"clear candidates!  candidates = []\")\n",
    "    for first_word, remain_word in splits:\n",
    "        #pdb.set_trace()\n",
    "        first_prob = math.log10(conditionalProb(first_word, prev))\n",
    "        #print(\"first_prob of condProb(\", first_word, \", \", prev, \"): \", first_prob )\n",
    "        remain_prob, remain_word = viterbi(remain_word, first_word)\n",
    "        #print(\"remain_prob: \", remain_prob, \", remain_word: \", remain_word)\n",
    "        candidates.append((first_prob + remain_prob, [first_word] + remain_word))\n",
    "        #print(\"first_prob: \", str(first_prob), \", remain_prob: \", remain_prob, \", [first_word]:\", [first_word], \", remain_word: \", remain_word)\n",
    "        #print(\"Candidates: \", candidates)\n",
    "        \n",
    "    #print(\"max(candidates): \" + str(max(candidates)))\n",
    "    #print(\"====================\")\n",
    "    return max(candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    wordDelimiter= '|' # assign local variable delimiter\n",
    "\n",
    "    input      = text[:]\n",
    "    listString = viterbi(input.replace(\" \", \"\").strip()) # remove space between words and pass to viterbi()\n",
    "    wordStr    = wordDelimiter.join(listString[1])\n",
    "    wordClean1 = wordStr.strip()\n",
    "    wordClean2 = wordClean1.strip(wordDelimiter)    \n",
    "    wordClean2 = wordClean2.split('|')                \n",
    "    return wordClean2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'The TimesOnline reports that the joke fell flat with Jeffrey Turner, who as Chief of Police in Clayton County, Georgia, put Mr Whitton on medical leave when he was shot in the wrist as he tried to foil a robbery earlier this summer.',\n",
       " 'my': 'ယခု နွေရာသီ အစောပိုင်း ကာလ ၌ လုယက်မှု တစ်ခု ကျူးလွန် ရန် ကြိုးစားခဲ့ သောကြောင့် လက်ကောက်၀တ် တွင် သေနတ်ကျည်မှန်ခဲ့သော မစ်စတာ ၀ှစ်တွန် ကို ကလေတွန် ကောင်တီ ၊ ဂျော်ဂျီယာပြည်နယ် မှ ၊ ရဲမှူးကြီး ဂျက်ဖရီ တာနာ မှ ဆေး ခွင့် ပေးတာနှင့် ပတ်သတ်ပြီး ၊ တိုင်းမ်စ်အွန်လိုင်း မှ ဟာသပြက်လုံးတစ်ခု ကို မှတ်တမ်းတင်ရေးသားခဲ့သည် ။'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[TRG_LANGUAGE] = my_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  ယခု နွေရာသီ အစောပိုင်း ကာလ ၌ လုယက်မှု တစ်ခု ကျူးလွန် ရန် ကြိုးစားခဲ့ သောကြောင့် လက်ကောက်၀တ် တွင် သေနတ်ကျည်မှန်ခဲ့သော မစ်စတာ ၀ှစ်တွန် ကို ကလေတွန် ကောင်တီ ၊ ဂျော်ဂျီယာပြည်နယ် မှ ၊ ရဲမှူးကြီး ဂျက်ဖရီ တာနာ မှ ဆေး ခွင့် ပေးတာနှင့် ပတ်သတ်ပြီး ၊ တိုင်းမ်စ်အွန်လိုင်း မှ ဟာသပြက်လုံးတစ်ခု ကို မှတ်တမ်းတင်ရေးသားခဲ့သည် ။\n",
      "Tokenization:  ['ယခု', 'နွေရာသီ', 'အစောပိုင်း', 'ကာလ', '၌', 'လုယက်', 'မှု', 'တစ်', 'ခု', 'ကျူးလွန်', 'ရန်', 'ကြိုးစား', 'ခဲ့', 'သော', 'ကြောင့်', 'လက်', 'ကောက်', '၀', 'တ်', 'တွင်', 'သေနတ်ကျည်', 'မှန်', 'ခဲ့', 'သော', 'မ', 'စ်', 'စ', 'တာ', '၀ှစ်', 'တွန်', 'ကို', 'က', 'လေ', 'တွန်', 'ကောင်', 'တီ', '၊', 'ဂျော်ဂျီယာ', 'ပြည်နယ်', 'မှ', '၊', 'ရဲမှူးကြီး', 'ဂျက်', 'ဖရီ', 'တာ', 'နာ', 'မှ', 'ဆေး', 'ခွင့်', 'ပေး', 'တာ', 'နှင့်', 'ပတ်', 'သတ်', 'ပြီး', '၊', 'တိုင်းမ်စ်', 'အွန်လိုင်း', 'မှ', 'ဟာသ', 'ပြက်လုံး', 'တစ်', 'ခု', 'ကို', 'မှတ်တမ်းတင်', 'ရေး', 'သား', 'ခဲ့', 'သည်', '။']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the myanmar part\n",
    "print(\"Sentence: \", sample[TRG_LANGUAGE])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE](sample[TRG_LANGUAGE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone datasetENMY for use later\n",
    "corpus = copy.deepcopy(datasetENMY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be 'train' or 'val' or 'test' \n",
    "def yield_tokens(data, language):\n",
    "    # language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE:1}\n",
    "    \n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language])\n",
    "        # either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "#     # Create torchtext's Vocab object \n",
    "#     vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "#                                                     min_freq = 2,   # if not, everything will be treated as UNK\n",
    "#                                                     specials = special_symbols,\n",
    "#                                                     special_first = True) # indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# # Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "# for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "#     vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocab with pickle for Use later\n",
    "\n",
    "# pickle.dump(vocab_transform, open('../app/data/vocab.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab is saved and loaded for further use and to save time for rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocab\n",
    "vocab_transform = pickle.load(open('../app/data/vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[880, 18, 12, 0, 12]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example of english token\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 1134, 16, 20]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example of myanmar token\n",
    "vocab_transform[TRG_LANGUAGE](['သူ', 'နွေရာသီ', 'တစ်', 'ခု'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 880, for example\n",
    "mapping[880]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16912"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2 Experiment: Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids, device):\n",
    "    return torch.cat((torch.tensor([SOS_IDX], device=device), \n",
    "                      torch.tensor(token_ids, device=device), \n",
    "                      torch.tensor([EOS_IDX], device=device)))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln],  # Tokenization\n",
    "                                               vocab_transform[ln],  # Numericalization\n",
    "                                               lambda x: tensor_transform(x, device=device))  # Add BOS/EOS and create tensor\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_batch(batch, device):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for row in batch:\n",
    "\n",
    "        # Check if the required fields are not None\n",
    "        if row[SRC_LANGUAGE] is None or row[TRG_LANGUAGE] is None:\n",
    "            print(f\"Skipping row with missing data: {row}\")  # Optional: print to debug\n",
    "            continue  # Skip this row if it's missing source or target data\n",
    "        \n",
    "        processed_text = text_transform[SRC_LANGUAGE](row[SRC_LANGUAGE].rstrip(\"\\n\"))\n",
    "        trg_text = text_transform[TRG_LANGUAGE](row[TRG_LANGUAGE].rstrip(\"\\n\"))\n",
    "        \n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(trg_text)\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    # Pad sequences and ensure they're on the correct device\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True).to(device) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True).to(device)\n",
    "    \n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64, device=device), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train   = corpus['train'][:2000] #First 2000 | 10%\n",
    "val     = corpus['validation'][:100] #First 100 | 10%\n",
    "test    = corpus['test'][:100] #First 100 | 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: collate_batch(batch, device))\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=lambda batch: collate_batch(batch, device))\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=lambda batch: collate_batch(batch, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x204083601a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, my in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 53])\n",
      "Myanmar shape:  torch.Size([64, 86])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Myanmar shape: \", my.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device, attention):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        self.self_attention       = attention(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, attention, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device, attention)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGeneralAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplicative Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadMultiplicativeAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.W        = nn.Linear(self.head_dim, self.head_dim) # for decoder input_ (note: not the same)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "    \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        # src, src, src, src_mask\n",
    "        # query = [batch size, query len, hid dim]\n",
    "        # key   = [batch size, key len, hid dim]\n",
    "        # value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        # Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        # update the size for matrix multiplication\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        # calculate the energy/ attention score\n",
    "        energy = torch.matmul(self.W(Q), K.permute(0,1,3,2)) / self.scale # multiplicative attention\n",
    "        # Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        # energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        # for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        # attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        # [batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        # x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        # x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        # x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        # x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additive Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAdditiveAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        # input >> Q, K, V\n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        # for additive v, U, W\n",
    "        self.vv = nn.Linear(self.head_dim, 1, bias = False)\n",
    "        self.W = nn.Linear(self.head_dim, self.head_dim) # for decoder input_ (W2)\n",
    "        self.U = nn.Linear(self.head_dim, self.head_dim)  # for encoder_outputs (W1)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "    \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        # src, src, src, src_mask\n",
    "        # query = [batch size, query len, hid dim]\n",
    "        # key   = [batch size, key len, hid dim]\n",
    "        # value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        query_len  = query.shape[1]\n",
    "        key_len    = key.shape[1]\n",
    "        \n",
    "        # linear transform for the input\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        # Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        # update the size for matrix multiplication\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q = [batch_size, n heads, query len, head_dim]\n",
    "    \n",
    "        Q = Q.view(batch_size, self.n_heads, query_len, 1, self.head_dim)\n",
    "        K = K.view(batch_size, self.n_heads, 1, key_len, self.head_dim)\n",
    "        \n",
    "        # calculate the energy : for additive\n",
    "        energy = (self.vv(torch.tanh(self.W(Q) + self.U(K))) / self.scale).squeeze(4)\n",
    "        # Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        # energy = [batch_size, n heads, query len, key len, 1] >> squeeze(4)\n",
    "        # energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        # for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        # attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        # [batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        # x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        # x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        # x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        # x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device, attention):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = attention(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = attention(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device, attention, max_length = 500):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device, attention)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(16912, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAdditiveAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (vv): Linear(in_features=32, out_features=1, bias=False)\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(8029, 256)\n",
       "    (pos_embedding): Embedding(500, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAdditiveAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (vv): Linear(in_features=32, out_features=1, bias=False)\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAdditiveAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (vv): Linear(in_features=32, out_features=1, bias=False)\n",
       "          (W): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (U): Linear(in_features=32, out_features=32, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=8029, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set Parameters for my models\n",
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "# Initialize Encoder and Decoder for General attention\n",
    "enc_general_attention = Encoder(input_dim, hid_dim, enc_layers, enc_heads, enc_pf_dim, enc_dropout, device,attention = MultiHeadGeneralAttentionLayer)\n",
    "dec_general_attention = Decoder(output_dim, hid_dim, dec_layers, dec_heads, dec_pf_dim, enc_dropout, device,attention = MultiHeadGeneralAttentionLayer)\n",
    "\n",
    "# Initialize Encoder and Decoder for Multiplicative attention\n",
    "enc_multiplicative_attention = Encoder(input_dim, hid_dim, enc_layers, enc_heads, enc_pf_dim, enc_dropout, device,attention = MultiHeadMultiplicativeAttentionLayer)\n",
    "dec_multiplicative_attention = Decoder(output_dim, hid_dim, dec_layers, dec_heads, dec_pf_dim, enc_dropout, device,attention = MultiHeadMultiplicativeAttentionLayer)\n",
    "\n",
    "# Initialize Encoder and Decoder for Additive attention\n",
    "enc_additive_attention = Encoder(input_dim, hid_dim, enc_layers, enc_heads, enc_pf_dim, enc_dropout, device,attention = MultiHeadAdditiveAttentionLayer)\n",
    "dec_additive_attention = Decoder(output_dim, hid_dim, dec_layers, dec_heads, dec_pf_dim, enc_dropout, device,attention = MultiHeadAdditiveAttentionLayer)\n",
    "\n",
    "# Create General attention model\n",
    "model_general_attention      = Seq2SeqTransformer(enc_general_attention, dec_general_attention, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model_general_attention.apply(initialize_weights)\n",
    "\n",
    "# Create Multiplicative attention model\n",
    "model_multiplicative_attention = Seq2SeqTransformer(enc_multiplicative_attention, dec_multiplicative_attention, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model_multiplicative_attention.apply(initialize_weights)\n",
    "\n",
    "# Create Additive attention model\n",
    "model_additive_attention     = Seq2SeqTransformer(enc_additive_attention, dec_additive_attention, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model_additive_attention.apply(initialize_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dict to store different models for easy access, iteration in future\n",
    "all_models = {\n",
    "    'generalAttention': model_general_attention,\n",
    "    'multiplicativeAttention': model_multiplicative_attention,\n",
    "    'additiveAttention' : model_additive_attention\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______\n",
      "12658013\n",
      "______\n",
      "12667517\n",
      "______\n",
      "12677309\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    # for item in params:\n",
    "    #     print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "# count_parameters(model)\n",
    "for i,model in enumerate(all_models):\n",
    "    count_parameters(all_models[model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set learning rate\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters for general attention\n",
    "optimizer_general_attention = optim.Adam(model_general_attention.parameters(), lr=lr)\n",
    "criterion_general_attention = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX) #combine softmax with cross entropy\n",
    "\n",
    "#training hyperparameters for multiplicative attention\n",
    "optimizer_multiplicative_attention = optim.Adam(model_multiplicative_attention.parameters(), lr=lr)\n",
    "criterion_multiplicative_attention = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX) #combine softmax with cross entropy\n",
    "\n",
    "#training hyperparameters for additive attention\n",
    "optimizer_additive_attention = optim.Adam(model_additive_attention.parameters(), lr=lr)\n",
    "criterion_additive_attention = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(train_loader)\n",
    "val_loader_length   = len(valid_loader)\n",
    "test_loader_length  = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time % 60)\n",
    "    elapsed_millis = int((elapsed_time - int(elapsed_time)) * 1000)  # get milliseconds\n",
    "    return elapsed_mins, elapsed_secs, elapsed_millis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store records of model performance\n",
    "model_records = {}\n",
    "\n",
    "# Define a Custom Function to call for training all models\n",
    "def train_model(model, optimizer, criterion, model_name):\n",
    "    \n",
    "    best_train_loss = float('inf')\n",
    "    best_train_PPL  = float('inf')\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_PPL  = float('inf')\n",
    "    \n",
    "    num_epochs = 5\n",
    "    clip       = 1\n",
    "\n",
    "    save_path = f'../models/{model_name}.pt'\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "        valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "        \n",
    "        #for plotting\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs, epoch_millis = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_train_loss = train_loss\n",
    "            best_train_PPL  = math.exp(train_loss)\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_PPL = math.exp(valid_loss)\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s {epoch_millis}ms')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "        #lower perplexity is better\n",
    "\n",
    "    end = time.time()\n",
    "    total_mins, total_secs, total_millis = epoch_time(start, end)\n",
    "\n",
    "    print(f\"Total training time: {total_mins}m {total_secs}s {total_millis}ms\")\n",
    "\n",
    "    # save the records\n",
    "    model_records[model_name] = {'train_losses' : train_losses,\n",
    "                               'valid_losses'   : valid_losses,\n",
    "                               'best_train_loss': best_train_loss,\n",
    "                               'best_train_PPL' : best_train_PPL,\n",
    "                               'best_valid_loss': best_valid_loss,\n",
    "                               'best_valid_PPL' : best_valid_PPL,\n",
    "                               'total_mins'     : total_mins,\n",
    "                               'total_secs'     : total_secs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: General Attention (0.5 Point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 54s 224ms\n",
      "\tTrain Loss: 5.724 | Train PPL: 306.045\n",
      "\t Val. Loss: 5.759 |  Val. PPL: 317.030\n",
      "Epoch: 02 | Time: 1m 55s 481ms\n",
      "\tTrain Loss: 5.348 | Train PPL: 210.171\n",
      "\t Val. Loss: 5.560 |  Val. PPL: 259.854\n",
      "Total training time: 3m 49s 846ms\n"
     ]
    }
   ],
   "source": [
    "# Run the custom training function for general attention model\n",
    "train_model (model_general_attention, optimizer_general_attention, criterion_general_attention, \"generalAttention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Multiplicative Attention  (0.5 Point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the custom training function for Multiplicative attention model\n",
    "train_model (model_multiplicative_attention, optimizer_multiplicative_attention, criterion_multiplicative_attention, \"multiplicativeAttention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Additive Attention  (0.5 Point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the custom training function for Additive attention model\n",
    "train_model (model_additive_attention, optimizer_additive_attention, criterion_additive_attention, \"additiveAttention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4: Model Comparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Funciton to test models\n",
    "def test(model, model_name, criterion):\n",
    "    save_path = f'../models/{model_name}.pt'\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "    print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test General Attention Model\n",
    "test(model_general_attention, \"generalAttention\", criterion_general_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multiplicative Attention Model\n",
    "test(model_multiplicative_attention, \"multiplicativeAttention\", criterion_multiplicative_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Additive Attention Model\n",
    "test(model_additive_attention, \"additiveAttention\", criterion_additive_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "mapping[SRC_LANGUAGE] = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "mapping[TRG_LANGUAGE] = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform greedy decoding to generate a translation for the given source sequence\n",
    "def greedy_decode(model, src_text, max_len, device):\n",
    "    src_text = text_transform[SRC_LANGUAGE](src_text).to(device).unsqueeze(0)\n",
    "    \n",
    "    src_mask    = model.make_src_mask(src_text)\n",
    "    memory      = model.encoder(src_text, src_mask)\n",
    "    trg_indexes = [SOS_IDX]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, _ = model.decoder(trg_tensor, memory, trg_mask, src_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        if pred_token == EOS_IDX:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = [mapping[TRG_LANGUAGE][i] for i in trg_indexes]\n",
    "    return trg_tokens[1:]  # Exclude the <sos> token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load General Attention Model\n",
    "model_general_attention = model.load_state_dict(torch.load(f'../models/generalAttention.pt'))\n",
    "model_general_attention.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Load Multiplicative Attention Model\n",
    "model_multiplicative_attention = model.load_state_dict(torch.load(f'../models/multiplicativeAttention.pt'))\n",
    "model_multiplicative_attention.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Load Additive Attention Model\n",
    "model_additive_attention = model.load_state_dict(torch.load(f'../models/additiveAttention.pt'))\n",
    "model_additive_attention.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentence = test[100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing inference using General attention\n",
    "output_sentence = greedy_decode(model_general_attention, src_sentence, max_len=20, device=device)\n",
    "\n",
    "print(\"Predicted Sentence:\", ' '.join(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing inference using Multiplicative attention\n",
    "output_sentence = greedy_decode(model_multiplicative_attention, src_sentence, max_len=20, device=device)\n",
    "\n",
    "print(\"Predicted Sentence:\", ' '.join(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing inference using Additive attention\n",
    "output_sentence = greedy_decode(model_additive_attention, src_sentence, max_len=20, device=device)\n",
    "\n",
    "print(\"Predicted Sentence:\", ' '.join(output_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1 \n",
    "Compare the performance of these attention mechanisms in terms of translation accuracy, computational efficiency, and other relevant metrics. (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the records\n",
    "model_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2\n",
    "Provide performance plots showing training and validation loss for each type of attention mechanism (General, Multiplicative, and Additive). These plots will help in visualizing and comparing the learning curves of different attention models. (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure to plot all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop through the models and plot the train/validation losses\n",
    "for idx, (model_name, model) in enumerate(all_models.items()):\n",
    "    plt.subplot(len(all_models), 1, idx + 1)  # Create a subplot for each model\n",
    "    \n",
    "    # Access train and validation losses for the current model\n",
    "    train_losses = model_records[model_name]['train_losses']\n",
    "    valid_losses = model_records[model_name]['valid_losses']\n",
    "    \n",
    "    # Plot the losses\n",
    "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "    plt.plot(valid_losses, label='Validation Loss', color='orange')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f'{model_name} Model')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3\n",
    "Display the attention maps generated by your model. Attention maps are crucial for understanding how the model focuses on different parts of the input sequence while generating the translation. This visualization will offer insights into the interpretability of your model. (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = corpus['train'][0]['en']\n",
    "print(src_text)\n",
    "\n",
    "trg_text = corpus['train'][0]['my']\n",
    "print(trg_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = text_transform[SRC_LANGUAGE](src_text).to(device).unsqueeze(0)\n",
    "trg = text_transform[TRG_LANGUAGE](trg_text).to(device).unsqueeze(0)\n",
    "\n",
    "text_length = torch.tensor([src.size(0)]).to(dtype=torch.int64)\n",
    "text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    general_output, general_attentions = model_general_attention(src, trg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    multiplicative_output, multiplicative_attentions = model_multiplicative_attention(src, trg)\n",
    "\n",
    "with torch.no_grad():\n",
    "    additive_output, additive_attentions = model_additive_attention(src, trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(output):\n",
    "    output = output.squeeze(0)\n",
    "    output = output[1:]\n",
    "    output_max = output.argmax(1)\n",
    "    trg_tokens = ['<sos>'] + [mapping[TRG_LANGUAGE][token.item()] for token in output_max]\n",
    "    return trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#Custom function to display attention\n",
    "def display_attention(sentence, translation, attention, title):\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "\n",
    "    ax.tick_params(labelsize=10)\n",
    "\n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence\n",
    "\n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](src_text) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention map for general attention model\n",
    "trg_tokens = get_output(general_output)\n",
    "display_attention(src_tokens, trg_tokens, general_attentions, \"generalAttention\")\n",
    "print(trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention map for Multiplicative attention model\n",
    "trg_tokens = get_output(multiplicative_output)\n",
    "display_attention(src_tokens, trg_tokens, multiplicative_attentions, \"multiplicativeAttention\")\n",
    "print(trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention map for Additive attention model\n",
    "trg_tokens = get_output(additive_output)\n",
    "display_attention(src_tokens, trg_tokens, additive_attentions, \"additiveAttention\")\n",
    "print(trg_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4\n",
    "Analyze the results and discuss the effectiveness of the selected attention mechanism in translating between your native language and English. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
